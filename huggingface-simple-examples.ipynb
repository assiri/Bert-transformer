{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"![Hugging Face](https://miro.medium.com/max/2000/1*Z4mGaMsu34LfyE76QAi9qA.png)\n","metadata":{"id":"dCM2XIGlgNG-"}},{"cell_type":"markdown","source":"# Using HuggingFace\n\nIn this tutorial, we will learn how to use the various functionalities offered by [HuggingFace](https://huggingface.co/).\n\n## About Hugging Face\n\n- HuggingFace is 'On a mission to solve NLP, One commit at a time', as per their tagline. \n\n- The HuggingFace Transformer library is closing on 26K Stars on GitHub now and provides state-of-the-art Transformer Based Models, their pretrained weights and a lots more (as we will see today)\n\n- They recently released their Tokenisers library\n\n\nThey have originally used Rust, so that's an added advantage.","metadata":{"id":"jZyI24eXcEEJ"}},{"cell_type":"code","source":"!pip install transformers","metadata":{"id":"UCGc19nd3TZJ","outputId":"22e9c462-7c18-420e-c957-ff0002e1c935","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Pipeline\n\nhttps://github.com/huggingface/transformers#quick-tour-of-pipelines","metadata":{"id":"vXMZYhxZ-7H8"}},{"cell_type":"code","source":"from transformers import pipeline","metadata":{"id":"NTaHAhi14usQ","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sentiment Analysis","metadata":{"id":"5conX00349D8"}},{"cell_type":"code","source":"nlp = pipeline('sentiment-analysis')\nnlp('We are very happy to include pipeline into the transformers repository.')","metadata":{"id":"XsIp762F44pA","outputId":"7d4bcfe1-45ad-463c-c32b-51c91f699ad3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Question Answering","metadata":{"id":"d1-ibaH-5L6P"}},{"cell_type":"code","source":"nlp = pipeline('question-answering')\nnlp({\n    'question': 'What is my name ?',\n    'context': 'My name is Ahmed, I am working with HuggingFace'\n})","metadata":{"id":"SBqN6XDF5ASb","outputId":"99617d00-74d5-492e-85c2-34992accfafc","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Predicting Masks","metadata":{"id":"VGe199t65pVg"}},{"cell_type":"code","source":"nlp = pipeline('fill-mask')\nnlp('I hope you <mask> this video')","metadata":{"id":"2d6NvCks5NnW","outputId":"f2c61e3c-ee30-4f52-f80e-44592373557e","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## NER","metadata":{"id":"T4SW0acf81ZQ"}},{"cell_type":"code","source":"nlp = pipeline('ner')\nnlp('It is me, Ahmed, I am working with HuggingFace')","metadata":{"id":"5ZA4oFr-8PpL","outputId":"ecab7263-dc2b-4325-b462-82c67cded7a3","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Text Generation","metadata":{"id":"amASDwJA_P01"}},{"cell_type":"code","source":"import torch\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel","metadata":{"id":"tcbUda0I9cfN","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokeniser = GPT2Tokenizer.from_pretrained('gpt2')\nmodel = GPT2LMHeadModel.from_pretrained('gpt2')","metadata":{"id":"iNFL0TUwC4Dr","outputId":"3fead14e-28f0-44c9-aa34-47159192e580","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Single Word Prediction","metadata":{"id":"b77RqrJgDH-p"}},{"cell_type":"code","source":"device = \"cuda\"\n\ntext = \"let us see how this turns\"\nindexed_tokens = tokeniser.encode(text)\ntokens_tensor = torch.tensor([indexed_tokens])\nmodel.eval()\ntokens_tensor = tokens_tensor.to(device)\nmodel.to(device)","metadata":{"id":"fuYAjdLGC6VT","outputId":"68e823c4-36fe-4bd5-baa5-35c93f9b1666","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n  outputs = model(tokens_tensor)\n  predictions = outputs[0]\n\nprint(outputs[0].shape)\n\npredicted_index = torch.argmax(predictions[0, -1, :]).item()\npredicted_text = tokeniser.decode(indexed_tokens + [predicted_index])\n\nprint(predicted_text)","metadata":{"id":"PNylT-NaC9UB","outputId":"99f2b0c1-a295-41ab-86a0-8b2f2882680d","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Looping for multi-word","metadata":{"id":"BzxsYVVMDcZf"}},{"cell_type":"code","source":"chars = 0\ntext = \"i am very exited to present to you this\"\nwhile chars<50:\n  chars += 1\n  indexed_tokens = tokeniser.encode(text)\n  tokens_tensors = torch.tensor([indexed_tokens])\n  tokens_tensors = tokens_tensors.to(device)\n  with torch.no_grad():\n    outputs = model(tokens_tensors)\n    predictions = outputs[0]\n  predicted_index = torch.argmax(predictions[0,-1,:]).item()\n  text = tokeniser.decode(indexed_tokens + [predicted_index])\n\nprint(text)","metadata":{"id":"6FyWdPiNDZLN","outputId":"9cb87e0c-de39-499d-d364-a1b5828aa994","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Summarising text using HuggingFace","metadata":{"id":"uIXrfA4WIzNH"}},{"cell_type":"code","source":"text = 'Shakespeare occupies a position unique in world literature. Other poets, such as Homer and Dante, and novelists, such as Leo Tolstoy and Charles Dickens, have transcended national barriers, but no writer’s living reputation can compare to that of Shakespeare, whose plays, written in the late 16th and early 17th centuries for a small repertory theatre, are now performed and read more often and in more countries than ever before. The prophecy of his great contemporary, the poet and dramatist Ben Jonson, that Shakespeare “was not of an age, but for all time,” has been fulfilled. It may be audacious even to attempt a definition of his greatness, but it is not so difficult to describe the gifts that enabled him to create imaginative visions of pathos and mirth that, whether read or witnessed in the theatre, fill the mind and linger there. He is a writer of great intellectual rapidity, perceptiveness, and poetic power. Other writers have had these qualities, but with Shakespeare the keenness of mind was applied not to abstruse or remote subjects but to human beings and their complete range of emotions and conflicts. Other writers have applied their keenness of mind in this way, but Shakespeare is astonishingly clever with words and images, so that his mental energy, when applied to intelligible human situations, finds full and memorable expression, convincing and imaginatively stimulating. As if this were not enough, the art form into which his creative energies went was not remote and bookish but involved the vivid stage impersonation of human beings, commanding sympathy and inviting vicarious participation. Thus, Shakespeare’s merits can survive translation into other languages and into cultures remote from that of Elizabethan England.'","metadata":{"id":"_mvwkDMMKLFM","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import BartTokenizer, BartForConditionalGeneration, BartConfig\n\nmodel = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\ntokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')","metadata":{"id":"cR9aT7VBI8zs","outputId":"6d379e98-098c-45c5-8fe6-cd169c3378de","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"inputs = tokenizer.batch_encode_plus([text], max_length=1024, return_tensors='pt')\n\nsummary_ids = model.generate(inputs['input_ids'], num_beams=4, max_length=100, early_stopping=False)\n\nfor ids in summary_ids:\n    short = tokenizer.decode(ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n\n    print(len(text), len(short))\n    print(short)","metadata":{"id":"J1vh8uAGI8wF","outputId":"14604795-a426-4b4e-9fcd-cb9bb6ecc132","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"HuggingFace Tranformers: https://github.com/huggingface/transformers\n\nBART: https://arxiv.org/abs/1910.13461\n\nCurious Case of Neural Text Degeneration: https://arxiv.org/abs/1904.09751","metadata":{"id":"IsjTjYg9eFfV"}},{"cell_type":"code","source":"","metadata":{"id":"NMFtFXFQeYm1"},"execution_count":null,"outputs":[]}]}